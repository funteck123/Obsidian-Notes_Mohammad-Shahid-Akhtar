





























research question
- general
	- faster
	- smaller
	- combination of other modern techniques
	- agentic?
	- cot?
	- newer architecture?
	- jepa, mamba, diffusion in object detection
	- continual learning?
	- signal streaming 
	- novel + substantial + relevant
-  topic themes
	- general object detection
	- long video
	- object recognition 
	- vsr
	- wifi
	- audio
	- sensor 
	- edge device
	- application in other field
	- open vocabulary
	- zero, one or few shot
	- segmentation
- topic 1
	- long video
	- faster recognition
	- face recognition
	- attribute recognition
	- less memory 
	- faster inference
	- missing person finder?
	- intelligent frame skip - not frame wise but vector wise video representation? 
	- flash vstream or aks?
	- multitask head to classify 8 characteristics simultaneously with detection
	- baselines?
	- videoscan? vector wise skip
- topic 2
	- less memory 
	- faster inference
	- more accurate than sota and faster
	- same accuracy 
- t3
	- vsr
		- more accuracte
		- more general
		- auto agile ir
		- any kind of noise 
		- dehazing
		- denoise
		- derain
		- deflare
		- desnow
		- long video? speed up
- t4
	- wifi based object / pose detection system  
		- faster
		- more accurate
		- new architectural changes
		- 
- t5
	- person recognition
	- hundreds of parameters to uniquely identify people
	- biometrics?
	- more new dataset - usage - some guy had a great dataset with many labelled characteristics of people from 8 types atleast
- t6
	- ai avatar
		- video
		- audio
		- multimodal
		- pictures
		- complete person
		- instruction following
		- video game techniques
		- animations
		- intelligent ai avatar in world
		- sims but better lip and face sync 
		- voice cloning
		- face cloning
		- history cloning
- t7
	- person life trajectory
- t8
	- exxplainable ai
	- visualization
	- blackboxness removal
- t9
	- rlhf
	- grpo
	- planning
	- prompting techniques
	- cot
	- reasoning
	- state space
	- dpo
	- moe
	- ppo
- t10
	- review paper
		- all optmization techniques in realtime cv
		- all optmization techniques in long video cv
		- architectural evolution of object detection models
		- biometrics evolutions 
		- vsr
		- llm inference optimzation
- t11
	- get all possible sensors 
		- do faster more accurate infrenence and machine learning on niche sensors
- t12
	- dataset
		- atleast 5 big ones
		- videos
		- images
		- labelled
		- sensor data
		- wifi imaging
- t13 
	- imaging using all spectrums of light - review - application
	- bats or other animals
- t14
	- llm or langchain or rag in healthcare clinical data
	- what is langchain or rag
- t15
	- patent? how and on what
- t16 
	- agentic multimodal
	- using other forms of data to improve work
	- latest work in agentic
- t17 
	- formal math agentic lean improvement
	- better finetuning 
	- planning
	- reasoning
- t18 - to learn
	- federrated
	- recurrent
	- autoregressive
	- diffusion
	- vlm
	- llm
	- transformer
	- uformer
	- swin transformer
	- u net
	- wavelet
	- fourier transform
	- image as signals
	- dnn vs cnn
	- rnn 
	- lstm
	- wavelet kernels, learnlets) that enhance model interpretability, efficiency, and performance in handling non-stationary data, such as in fault diagnosis, image processing, and time-series forecasting
	- Wavelet-Based Attention and Feature Enhancement.
	- Wavelet-Based Multi-Agent and Reinforcement Learning
	- Wavelet Diffusion Neural Operators
	- _DiffusionDet_ #img [Chen\_DiffusionDet\_Diffusion\_Model\_for\_Object\_Detection\_ICCV\_2023\_paper.pdf](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.pdf)
	- DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing
	- building models
	- [\[2506.20302\] TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
	- # Quantization vs Pruning vs Distillation
	- new math in current ai architecture 
	- indescent
	- trees 
	- graphs
	- reservoir computing
	- gated echo
	- contrastive learning
- v19 
	- debias
		- auto
		- better 
		- faster
		- making llm forget
		- making or any model forget
		- genreal unlearning
		- unlearning
- v20
	- simplifying network structures



[Vision Transformers for Object Detection - Hugging Face Community Computer Vision Course](https://huggingface.co/learn/computer-vision-course/en/unit3/vision-transformers/vision-transformer-for-objection-detection)
[\[2004.09569\] Neural network compression via learnable wavelet transforms](https://arxiv.org/abs/2004.09569)

[Title Unavailable \| Site Unreachable](https://www.sciencedirect.com/science/article/pii/S1474034624003173)

- [Neehar Peri](http://www.neeharperi.com/) (PhD at CMU, co-advised by Deva Ramanan)
- [Anish Madan](https://scholar.google.com/citations?user=eZ4WZmIAAAAJ&hl=en) (MSCV at CMU, co-adivsed by Deva Ramanan)
- - [Zhiqiu Lin](https://linzhiqiu.github.io/) (PhD at CMU, co-advised by Deva Ramanan)
- - [Neehar Peri](http://www.neeharperi.com/) (RISS Fellow at CMU-RI, home institute UMD; now PhD at CMU)
- High-School Students
Sophie Zhou (Summer 2024, Cheyenne Mountain High School; joining Yale as undergrad)
Sarthak Jain (Summer 2022, Los Gatos High School; now undergrad at UIUC)

#imp ask to join research groups
[Shu Kong](https://aimerykong.github.io/group.html)





- dataset 
- eval


[foundational\_fsod/datasets/README.md at main · anishmadan23/foundational\_fsod · GitHub](https://github.com/anishmadan23/foundational_fsod/blob/main/datasets/README.md)

[GitHub - IT3DEgo/IT3DEgo: CVPR 2024 "Instance Tracking in 3D Scenes from Egocentric Videos"](https://github.com/IT3DEgo/IT3DEgo/)

[**Feature Selection from High-Order Tensorial Data via Sparse Decomposition**](http://www.sciencedirect.com/science/article/pii/S0167865512001985)

[**Learning Individual-Specific Dictionaries with Fused Multiple Features for Face Recognition**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553710&casa_token=c2VOuTB94nAAAAAA:th-WG1dls0p_vIS3t8-q0QSeKufxX5iKTMIvQrRK_6X51j9GR_kab3RZsEaoYy5HA0qPoQGP&tag=1)

>[**A Dictionary Learning Approach for Classification: Separating the Particularity and the commonality**](http://link.springer.com/chapter/10.1007%2F978-3-642-33718-5_14)



- find the older paper who won third place
- what is agile ir? recreate
- auto debias recreate


https://www.reddit.com/r/MachineLearning/comments/1ratkiz/d_is_this_what_ml_research_is/

track of current progress
- yolo person detection
	- Parallel Seek Strategy
	- Advanced GPU-CPU Parallelization
	- Asynchronous Producer-Consumer Pattern
	- Streaming Architecture with Frame Disposal
	- Zero-Copy Shared Memory Painters
	- Thread-Safe Dynamic Dictionaries
	- Kernel Auto-Tuning
	- NMS (Non-Maximum Suppression) Optimization
	- Quantization (Precision Reduction)
	- Gradient Checkpointing
	- Prompt Caching?? did i do it whats that
	- Stride Optimization (cap.grab vs. cap.retrieve)
	- VRAM Usage Monitoring and Management
	- NGINX Reverse Proxy & WebSocket Upgrade? not relevant to model
	- reid
	- frame stride
	- preprocess frame
	- mps
	- consumer-producer async cpu gpu
	- num cpu gpu tuning
	- benchmarking
	- tensorrt engine pt onnx hyper paramter tuning
	- layer fusion
	- model pruning
	- model export
	- different decoding libraries
	- gpu expansion memory tree param changes
	- multi threading
	- multi processing
- face recognition index generation
	- cpu gpu consumer producer work pool
	- multi threading 
	- multi processing
- index searching
- baseline
	- YOLOv8
	- YOLOv11
	- YOLOv26
	- RDETR
	- Grounding Dino
	- PALIGEMMA
	- grounding dino
	- dino x
	- YOLO World
	- CLIP
	- Rex-Omni
	- t rex and trex2
	- TAPTRv1
- techniques: 
	- add-on, architecture code change - learn archiecture, undersatnding embeddings
	- understand the math
	- loss functions 
- latest techniques
	- TAPTRV3: SPATIAL AND TEMPORAL CONTEXT FOSTER ROBUST TRACKING OF ANY POINT IN LONG VIDEO

[\[2312.14494\] Revisiting Few-Shot Object Detection with Vision-Language Models](https://arxiv.org/abs/2312.14494)

[\[2506.04713\] Enabling Validation for Robust Few-Shot Recognition](https://arxiv.org/abs/2506.04713)

[\[2507.03504\] Information-Bottleneck Driven Binary Neural Network for Change Detection](https://arxiv.org/abs/2507.03504)

#todo
course on research planning
track what i have done so far
list of papers to study
compare with others - what others are doing
propose new methods - copy from other fields - latest techniques
finish deeplearning book



[Fabiola Larios \| CVPR AI Art](https://thecvf-art.com/project/fabiola-larios-%e2%99%a1-cuteveillance-%e2%99%a1/)

#imp new models
JEPA
EBM
MAMBA

#imp 




- refdetr 
- transformer faster than cnn




[Home - ACM SIGAI](https://sigai.acm.org/main/)
[AI Matters - ACM SIGAI](https://sigai.acm.org/main/ai-matters/)


## 2025 SRC Winner Third Place, Graduate

**Vaastav Anand, Max Planck Institute for Software Systems**  
  
"Online Specialization of Systems with Iridescent" ([SOSP 2024](https://sigops.org/s/conferences/sosp/2024//))

1 Introduction Specializing system implementations to workload characteristics and hardware can significantly improve performance and efficiency [1, 3, 5–10, 13, 14, 18, 20–22, 25]. To achieve these benefits for particular hardware and workload combination, system developers manually modify the system code and recompile to benefit from the compile-time optimizations enabled by specialization [15, 23]. System specialization comes at the cost of generality.A system heavily specialized to workload and hardware either performs poorly outside of this regime, or completely fails. As a result, developers today must carefully navigate this specialization-generalization tradeoff and optimize for the most common hardware and workload setting.

# AI Matters

### Editor-In-Chief:

- Ella Scallan, Association for the Understanding of Artificial Intelligence

### Editorial Board:

- Sanmay Das, George Mason University
- Alexei Efros, University of California at Berkeley
- Susan L. Epstein, The City University of New York
- Yolanda Gil, University of Southern California
- Doug Lange, SPAWAR Systems Center, US Navy
- Kiri Wagstaff, JPL/Caltech
- Xiaojin (Jerry) Zhu, University of Wisconsin-Madison
- Nicholas Mattei, Tulane University

### Column Editors:

- Louise A. Dennis, University of Manchester
    
- Dilini Samarasinghe, University of New South Wales
    
- Dongkuan Xu, North Carolina State University
    

### Editor emerita:

- Kiri Wagstaff, JPL/Caltech (founding Editor-In-Chief)
- Eric Eaton, University of Pennsylvania
- Amy McGovern, University of Oklahoma

#imp sigai newsletter publication 
#### How to Submit

**AI Matters** is an electronic publication, published quarterly. We are keen to expand our newsletter to include more community-written content, so we are welcoming _any_ and _all_ types of contribution – whether you have a quick idea, a reflection, or a full piece ready to go. You can send us a short pitch or draft – we’re happy to help shape it together. We are also looking for regular columnists, so if you have an idea for a recurring feature, get in touch. 

Here are some examples of the kinds of pieces we publish (but don’t feel limited to these):

- **AI Impact:** Examples of AI systems or methods that have had a real-world impact.
- **Opinion:** Thought-provoking takes or discussions on issues in AI. Joint or response pieces are welcome.
- **Paper Précis:** Short summaries of recent AI papers, written for a general audience.
- **Event Reports:** Your reflections from conferences, workshops, or community events.
- **News:** Open-source software, datasets, challenges, or other updates of interest to AI researchers and practitioners.
- **Early Career Researcher Advice:** Are you an early career researcher, or do you have some advice for your younger colleagues? Send us your tips!
- **Ph.D. Dissertation Briefings:** Extended abstracts about recent dissertations, to increase visibility for new graduates.
- **Book Announcement:** Description of a newly published book and its major contributions.
- **Book Review:** Tell us about a book you find valuable to your AI work.
- **Tutorials:** Short introductions to AI concepts or techniques, for the benefit of students and those new to the field – text, images, or video all welcome.
- **Visual and Creative Pieces:** Videos, images, puzzles, jokes, or memorable AI encounters.
- **Personal Reflections and Observations:** This is a broad category – send us your short thoughts. What’s your favourite AI concept, and why? What do you think the next big research area in the field will be? Get in touch. 

Email your ideas or submissions to [aimatters@sigai.acm](mailto:aimatters@sigai.acm). We’ll reply and help you develop it for publication, if suitable. As well as reaching your ACM SIGAI colleagues, any relevant content can be posted to [AIhub.org](http://aihub.org/) and [Robohub.org](http://robohub.org/), so you can reach a wider audience. 

_Please note that calls for papers and other announcements for conferences or workshops are not appropriate for publication in the newsletter._ 

All contributions are edited for relevance, clarity, and suitable length.


AAAI conference

## AI Matters summary – last issue:

## [VOLUME 11, ISSUE 2, 2026](https://sigai.acm.org/main/2026/02/05/volume-11-issue-2-2026/)

### Full Issue

[Click here](https://sigai.acm.org/static/aimatters/11-2/January%2026%20-%20Vol%2011%20Issue%202.pdf) for the January 2026 issue
**1. [Table of Contents](https://sigai.acm.org/static/aimatters/11-2/1.%20Table%20of%20Contents.pdf)**

**2. [Letter from Editor](https://sigai.acm.org/static/aimatters/11-2/2.%20Letter%20from%20the%20Editor.pdf)**  
_By Ella Scallan_


#imp sig ai mentoring 
### E-mentoring Opportunity – Just 20 Minutes Per Week

The MentorNet One-on-One Mentoring Programs are a chance to make a big difference in the life of a student in as little as 20 minutes a week.

ACM is now partnering with MentorNet, an organization that promotes e-mentoring relationships between students (proteges) and professionals (mentors). Mentors and students communicate entirely by email, wherever and whenever they choose. The programs have proven effective by providing “real world” information, encouragement, advice, and access to networks for students, and particularly for those underrepresented in engineering and science fields.

MentorNet seeks science and engineering professionals to mentor engineering and science community college, undergraduate, and graduate students, who are interested in pursuing a professional future in the fields of engineering and science.

Since 1998, MentorNet has matched more than 20,000 pairs of proteges and mentors. Over 90% of participants would recommend MentorNet’s e-mentoring programs to a friend or colleague. Here is what one ACM mentor says about the program: “I have been a mentor with MentorNet for almost five years now. I have had a variety of mentees from graduate students who are struggling to decide if the PhD is the right thing for them to faculty members wanting to determine if moving up into administrative positions is the right thing for their career. Each of these mentees has brought a fresh set of questions and backgrounds that are unique to them. Each has had different issues that need to be dealt with and challenges that they face. Each of these mentees has challenged me to think about my career path and what things were important for me along the way. I enjoy immensely the feeling that I am providing a sounding board for young professionals as they advance their careers. It is very rewarding to have these mentees come back and say that you have helped them to learn about themselves and their career interests. It is a relatively small time investment for a huge personal reward.”

Donna Reese, Associate Dean for Academics and Administration James Worth Bagley College of Engineering, Mississippi State University.

To learn more, go to: [**MentorNet**](https://mentornet.org/).

##### A. M. Turing Award

ACM’s most prestigious technical award is given to an individual selected for contributions of a technical nature made to the computing community. The contributions should be of lasting and major technical importance to the computer field. A number of distinguished researchers from the AI Community, listed below, have been honored with the Turing Award. See also the [complete list of Turing Award winners.](http://awards.acm.org/homepage.cfm?awd=140)

- 2011 [Judea Pearl](http://amturing.acm.org/award_winners/pearl_2658896.cfm)
- 2010 [Leslie G. Valiant](http://amturing.acm.org/award_winners/valiant_2612174.cfm)
- 1994 [Feigenbaum, Edward A](http://amturing.acm.org/award_winners/feigenbaum_4167235.cfm), [Reddy, Raj](http://amturing.acm.org/award_winners/reddy_6247682.cfm)
- 1991 [Milner, A J (Robin)](http://awards.acm.org/citation.cfm?id=1569367&srt=all&aw=140&ao=AMTURING&yr=1991)*
- 1975 [Newell, Allen](http://amturing.acm.org/award_winners/newell_3167755.cfm)*, [Simon, Herbert A](http://amturing.acm.org/award_winners/simon_1031467.cfm)*
- 1971 [McCarthy, John](http://amturing.acm.org/award_winners/mccarthy_0239596.cfm)*
- 1969 [Minsky, Marvin](http://amturing.acm.org/award_winners/minsky_7440781.cfm)

#imp 
**submitting** to top conferences (CVPR, ICML) is almost always **FREE**.

#imp icann
Go back to the **ICANN 2026** (Padua, Italy) target.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]

- **Submission Cost:** €0.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
    
- **If accepted:** Apply for their "Student Travel Grant" immediately.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]


1. **Student Volunteer Programs (The #1 Way):**
    
    - **What it is:** You work for the conference for ~10 hours (checking badges, handing out microphones).
        
    - **Reward:** **Free Registration** (saves you 
        
        ```
        400−400−
        ```
        
        800).[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
        
    - **When to apply:** Usually 3–4 months before the conference.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)] Watch the conference website like a hawk.
        
2. **Travel Grants:**
    
    - Every major conference (CVPR, ICANN, ICCV) has a "Travel Grant" application.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
        
    - **Criteria:** They prioritize students from developing countries (Malaysia counts!) and underrepresented groups.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
        
    - **Reward:** They often cover your flight + hotel + registration.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
        
3. **The "Preprint" Strategy (Free Submission):**
    
    - Remember, **submitting** to top conferences (CVPR, ICML) is almost always **FREE**.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)] You only pay if you get accepted and attend.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
        
    - If you get accepted, your university will almost certainly pay for you to go.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)] If they won't, you can withdraw (but having "Accepted" on your CV is worth fighting for funding).[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
        

**Next Step for You:**  
Go back to the **ICANN 2026** (Padua, Italy) target.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]

- **Submission Cost:** €0.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
    
- **If accepted:** Apply for their "Student Travel Grant" immediately.[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFVx7cliT4IRzrHynqkugn3YMOCeTYYtCd2hIHKn9fwlZrVIgsHcoQS5Jn2l2SNVUisZ1OKp0CGuEtQiE36LBQ4G8qGMsHR4NcnG7dDaaGf0YOTX5VIdHSpAunhO0Nl)]
##strudent volunteer program
[ICSE 2026 - Student Volunteers - ICSE 2026](https://conf.researchr.org/track/icse-2026/icse-2026-student-volunteers)
## [Benefits](https://conf.researchr.org/track/icse-2026/icse-2026-student-volunteers#benefits)

- Access to the 7 days of the conference, formally registered to the main conference only (including the social events and meals). Note that Student Volunteer registration does not cover papers (except for the ACM SRC and DS). For more information, check the Registration page: [https://conf.researchr.org/attending/icse-2026/registration](https://conf.researchr.org/attending/icse-2026/registration);


##research field
Movement using computer vision - robotics, self driving cars

# CVPR Art Gallery 2026

Computer vision techniques have long been used to make art, with tools such as GANs, diffusion models, and NeRF gaining popularity in recent years to create novel aesthetics, imagine new ways of seeing the world and investigate the potential of human-machine collaboration in the arts. In addition to that, artists have been looking critically into the workings and applications of object and facial recognition, producing works that highlight the limitations of the current state of technology, critique the machine’s view of the world and use the technology in unexpected ways.

At CVPR 2026, we will present an exhibition of artworks at Denver, Colorado and in an online gallery at [thecvf-art.com](https://thecvf-art.com/). We encourage submissions from anyone interested in AI art, whether you’re a computer vision researcher with visually engaging results or a professional artist. Submissions may include, but are not restricted to:

- Artworks created with novel computer vision techniques
- Artworks created using established computer vision techniques in original or unexpected ways
- Artworks presenting a critical or alternative perspective on computer vision techniques and applications

## **Timeline**

#imp
## cvitw 2026 

The 5th Workshop on Computer Vision in the Wild  
**Theme**: Building Multimodal AI Agents with Verbal, Spatial and Temporal Intelligence  
Date: June 3-4 | Location: Denver Convention Center, Denver CO
`Workshop Paper Submission Portal: [[Open Review]](https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/CVinW)![](https://computer-vision-in-the-wild.github.io/cvpr-2026/static/cvpr2025/img/icons/icon_live.png)   ``Submission Deadline: May 16th, 2026   Acceptance Notification: May 23rd, 2026   Camera-ready Submission May 30th, 2026`


##study note
the temptation to cognitive offload effort - quick and easy - focus on process and not product - already constant partial attention - human critical capacity declines - sustained intellectual exertion - Lindy effect - antifragile effect - introduce good friction in life - constrained ai - using ai and tech to execise the brain rather than replace it - adpation works best when we make a conscious effect  - many people want you to believe that its all doom and gloom so you dont learn the potential - device free reading or writing hours - boredom - pleasure - leaisure - f2f conversation - coding without ai - every idle moment is not idle moment -- intellectual breakthough often comes during unsructured time - boredom leads to ideation

##note
[Springer Nature](https://communities.springernature.com/channels/events) publishes proceedings for numerous international conferences across science, technology, medicine, and social sciences, notably through series like **Lecture Notes in Computer Science (LNCS)**, **LNBIP**, and **CCIS**.

##cvpr art
Deadline for art submissions: Mar 08, 2026 11:59 PM AOE or 02 weeks 02 days 00:41:05

Art acceptance notification: Mar 27, 2026 AOE

Art Gallery Exhibition: Jun 5 - 7, 2026


#imp
Nature Scientific Reports or IEEE Trans. on PAMI

self knowledge score:
6/100

It was great to host the visit of Prof. [**Costas Soutis FREng**](https://www.linkedin.com/in/costas-soutis-freng-a9aa004/) to our lab and show him our new lightweight & reconfigurable FlexPal soft robotic arm (mainly developed by my PhD student, [**Yinan Meng**](https://www.linkedin.com/in/yinan-meng1/)) together with other projects.  
  
[**#softrobot**](https://www.linkedin.com/search/results/all/?keywords=%23softrobot&origin=HASH_TAG_FROM_FEED) [**#3Dprinting**](https://www.linkedin.com/search/results/all/?keywords=%233dprinting&origin=HASH_TAG_FROM_FEED) [**#composites**](https://www.linkedin.com/search/results/all/?keywords=%23composites&origin=HASH_TAG_FROM_FEED) [**#additivemanufacturing**](https://www.linkedin.com/search/results/all/?keywords=%23additivemanufacturing&origin=HASH_TAG_FROM_FEED) [**#computationaldesign**](https://www.linkedin.com/search/results/all/?keywords=%23computationaldesign&origin=HASH_TAG_FROM_FEED)


Aigent

[LinkedIn Login, Sign in \| LinkedIn](https://www.linkedin.com/showcase/machines-can-see/posts/?feedView=all)
[hashtag#MCT2026](https://www.linkedin.com/search/results/all/?keywords=%23mct2026&origin=HASH_TAG_FROM_FEED) AI Summit

Mark Cuban Foundation AI Bootcamps
](https://x.com/markcubanai)
[https://x.com/markcubanai/status/1324737613077643264](https://x.com/markcubanai/status/1324737613077643264)

deva ramanan
Robotics Institute at Carnegie Mellon University

[RF-DETR: neural architecture search for real-time detection transformers](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&cstart=100&pagesize=100&citation_for_view=9B8PoXUAAAAJ:jtI9f0ekYq0C)

[Are we asking the right questions in MovieQA?](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&cstart=100&pagesize=100&citation_for_view=9B8PoXUAAAAJ:uWiczbcajpAC)

[Volumetric correspondence networks for optical flow](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&cstart=20&pagesize=80&citation_for_view=9B8PoXUAAAAJ:kuK5TVdYjLIC)

[Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&cstart=20&pagesize=80&citation_for_view=9B8PoXUAAAAJ:nb7KW1ujOQ8C)

[Simultaneous map and dynamic object reconstruction from lidar](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&sortby=pubdate&citation_for_view=9B8PoXUAAAAJ:OzeSX8-yOCQC)

WIFI to color construction - color image

[RF-DETR: neural architecture search for real-time detection transformers](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&sortby=pubdate&citation_for_view=9B8PoXUAAAAJ:jtI9f0ekYq0C)

[Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&sortby=pubdate&citation_for_view=9B8PoXUAAAAJ:37UQlXuwjP4C)

[Mapanything: Universal feed-forward metric 3d reconstruction](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&sortby=pubdate&citation_for_view=9B8PoXUAAAAJ:YTuZlYwrTOUC)

[Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9B8PoXUAAAAJ&sortby=pubdate&citation_for_view=9B8PoXUAAAAJ:hGdtkIFZdKAC)



Deva Ramanan

**Two-Pass Adaptive Inference (Sept 2025):** * **The Concept:** A model-independent approach that uses a very tiny "Pass 1" model to identify high-interest regions, then runs your main YOLOv26/RT-DETR only on those specific crops.

### Your Path to a "Nature Reports" Paper:

To move from "Intern Project" to "Nature Publication," your paper should be structured like this:

1. **The System:** Introduce your **High-Throughput Asynchronous Pipeline** (The 500 FPS work).
    
2. **The Innovation:** Add an **Adaptive Sampler** (like AKS) so the pipeline only "wakes up" when it sees a small object or motion.
    
3. **The Baseline:** Compare **YOLOv26** vs. **RT-DETR** using your pipeline on the **LongVideoBench** or **VisDrone** datasets.
    
4. **The Result:** Show that your "Adaptive Consumer-Producer" framework is **3x faster** than the standard implementation with **0% loss in accuracy**.




**Research Gap:** You can integrate this into your pipeline. Instead of a fixed stride, your "Producer" can use a Pass 1 model to dynamically decide which frames (or parts of frames) the "Consumer" should process.

**Concurrent Multi-Frame Processing (Jan 2025):** * **The Concept:** A scheme that groups frames according to GPU cores and processes them in parallel without synchronization. This is the academic version of your Consumer-Producer pipeline.

- **Innovation:** Researchers achieved a **445% runtime improvement** by overcoming the "serial processing" bottleneck of standard YOLO implementations on Jetson/Orin devices.

- **ZERO: Industry-Ready Vision Foundation Model (July 2025):** * **The Niche:** This model uses "Visual Exemplars" (one-shot). You give it one image of a person, and it finds them everywhere in 24 hours of footage without retraining. It took 2nd place at CVPR 2025 for "Object Instance Detection".
    
- **Prompt-Free Visual Embedding (2026):**
    
    - **The Niche:** New modules like **SAVE (Self-Attention on Visual Embedding)** can count or find objects without any text prompt, just by looking for "repetitive semantic patterns." This is the ultimate "zero-shot" search.
**Kernel Auto-Tuning & Layer Fusion (2026 Optimization):** * **The Concept:** Using TensorRT to merge multiple network layers into a single CUDA kernel. This reduces the time the GPU spends reading/writing to memory.

Since your goal is 24-hour footage, you should focus on **Temporal Redundancy**—the idea that frame 1000 is usually 99% the same as frame 1001.

- **T3S: Test-Time Temporal Sampling (Nov 2025):**
    
    - **The Concept:** A universal, zero-training sampling method. It breaks a long video into short subsequences and processes them with minimal integration effort. It has been shown to speed up models like Qwen2 and YOLO-based VLMs by **2.04x**.
        
- **AKS: Adaptive Keyframe Sampling (CVPR 2025):**
    
    - **The Concept:** A plug-and-play module that ranks frames based on "Relevance" and "Coverage." It ignores the boring parts of a 24-hour video and only feeds the important frames to your detector.
        
- **Motion Vector Propagation (MVP):** Instead of running YOLO on every frame, you run it on frame 1, and then use the H.265 "motion vectors" (the math that already exists in the video file) to "drag" the boxes to frame 2, 3, and 4. This is **10x faster** than re-running the model.

### 1. Who is building "Fast & Small" Pipelines in 2025?

These researchers are focused on the same "production speed" problems you solved in your internship, particularly for small objects and surveillance.

- **YOLO26 (Ultralytics & Cornell, Sept 2025):** This is the direct successor to the YOLO family you mentioned. It focuses on **"Architecture Simplification"**—removing the very bottlenecks you likely faced. It is specifically designed for edge devices and low-power hardware, achieving higher accuracy for small objects by simplifying the "Neck" of the network.
    
- **Improved YOLOv10 for Kitchen/Industrial Safety (MDPI, Jan 2025):** This work mirrors your "Missing Person Finder." They focused on **small and occluded targets** (like kitchen safety equipment) and achieved a 7.7% mAP increase while maintaining real-time speeds (12ms inference). They used a "Multi-module Optimization Framework" similar to how you integrated different AI modules.
    
- **BehaveAI (PLoS Biology, Feb 2026):** This is a brilliant example of what I mentioned earlier—applying high-speed CV to another science. They use **YOLO11** and a "color-from-motion" strategy to detect tiny behaviors in organisms that are invisible in static frames. This is a 2026 paper that reached a top-tier biology journal.
    

### 2. Who is solving the "24-Hour Video" Problem?

This is where you can find "Research Worthy" gaps. Most people are moving away from processing every frame and toward **Adaptive Sampling**.

- **AKS: Adaptive Keyframe Sampling (CVPR, 2025):** * **The Idea:** Instead of your "fixed stride" (every 30th frame), this research uses a Vision-Language Model (VLM) to "glance" at the video and pick the most relevant frames to process.
    
    - **The Gap for You:** You can combine your **500 FPS pipeline** with their **AKS algorithm**. While they focus on accuracy for long videos, they are often slow. Your research could be making _their_ adaptive sampling run at your production speeds.
        
- **GenS: Generative Frame Sampler (ACL, 2025):** This uses a "plug-and-play" module to identify question-relevant frames in hours-long footage. It is currently the state-of-the-art for "HourVideo" benchmarks.
    
- __T_ (T-Star) Temporal Search (CVPR, 2025):_* This researchers re-framed long-video understanding as a **spatial search**. They "zoom in" on specific temporal regions of a video to recover small object details without processing the entire 24-hour stream.
    

### 3. The "Holy Grail" for your Nature Paper: Compressed Domain Detection

Since you were bothered by the "24-hour footage" processing time, look at **ResearchGate's latest 2025/2026 surveys on HEVC/H.265 Compressed Domain Detection**.

- **Innovation:** These models detect vehicles or people **without ever decoding the video** into pixels. They use the "Motion Vectors" already inside the video file.
    
- **The SOTA:** A 2025 paper showed a **56% increase in speed** over traditional decoding methods while maintaining 99.9% accuracy for vehicle detection.
    
- **Your Path:** If you can adapt your "Consumer-Producer" pipeline to work directly on **H.265 bitstreams**, you would have a legitimate shot at a high-impact publication like _Nature Scientific Reports_.
    

### Summary Checklist for your Research Paper:

1. **Don't just use YOLOv8/11:** Use **YOLO26** or **RF-DETR** as your baseline (they are the 2025 speed kings).
    
2. **Abandon "Fixed Stride":** Replace it with a "Lightweight Motion-Triggered Sampler" or "Adaptive Keyframe Selection" (like the **AKS** paper).
    
3. **Benchmark on "VisDrone" or "LongVideoBench":** Private CCTV footage won't get you into _Nature_. You need to prove your pipeline beats the current SOTA on these public datasets.
    
4. **Target Niche:** Frame it as **"Resource-Constrained Surveillance"** or **"Carbon-Efficient Video Analytics."** Academics love "Accuracy-per-Watt" right now.



**"MVP: Motion Vector Propagation for Zero-Shot Video Object Detection" (Late 2024/2025):** This paper solves your exact problem. Instead of running heavy detectors on every frame, they run it once, and then use the video's compressed motion vectors to "drag" the bounding boxes across the next 30 frames.

**"MovieChat+: Question-Aware Sparse Memory for Long Video Question Answering" (IEEE):** Shows how to manage RAM/VRAM when dealing with millions of frames over long periods.

- **"EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding" (NeurIPS):** Read this to understand how academics define and measure "long-form" vs "short-form" video tasks.


**"Selective Structured State-Spaces for Long-Form Video Understanding" (CVPR):** Explores how to use a mask generator to drop useless frames/tokens in long videos to save memory and compute.


- **YOLOv10 / YOLO11 (Ultralytics):** Use these as your primary speed benchmarks.
    
- **"Focus on what you want to learn" (YOLOv9/v10):** These papers discuss removing NMS (Non-Maximum Suppression) to increase speed—similar to your goal of removing bottlenecks.
    
- **"SkyScanner" or "Glimpse":** Research papers that focus on skipping redundant frames in long-form video to save compute.

**RT-DETR:** You already experimented with this. Frame your research as "Why YOLO + your Pipeline beats Transformers for long-form surveillance."

- **VisDrone:** Essential for small-object detection (objects are tiny and numerous).
    
- **Argos:** A large-scale surveillance dataset specifically for long-term tracking.
    
- **Methodology:** You must report not just **mAP** (accuracy) and **FPS** (speed), but also **Latency-Accuracy Trade-off Curves** and **Power Consumption (Joules per frame)**. Nature-level reports value efficiency (accuracy per watt).

Glimpse-based Object Detection


**UC Berkeley**_Efficient VLMs_Professor Trevor Darrell's group is focusing on making **Vision-Language Models (VLMs)** smaller so they can reason about objects (e.g., "find the small red wire") in real-time.

**Stanford (HAI)**_Data-Efficient SOD_Researching how **Self-Supervised Learning** can help models detect objects they have only seen a few times (crucial for rare medical anomalies or specialized military hardware).


**MIT CSAIL**_Hardware-Aware AI_Developing "Low-Power Vision Transformers" that use **40% less energy** while maintaining high accuracy for small-object medical imaging.

[PhD: How to write a great research paper - YouTube](https://www.youtube.com/watch?v=1AYxMbYZQ1Y)


[How to write your first computer science research paper? - YouTube](https://www.youtube.com/watch?v=j1DvCavAmhE)

[Adaptive Background Estimation - Parking Lot - YouTube](https://youtu.be/z1n6jz-clPA?si=BsOpxYigX0REobxp)
explainable ai
lang chain
rag
mamba
state space
latest yolo v26 paper

World model - robotics
- [Generative AI Handbook: Statistical Prediction and Supervised Learning & Preliminaries (genai01 1 2) - YouTube](https://www.youtube.com/watch?v=VylAhVEf_64&list=PL3x6DOfs2NGivdWYQXgxTuAY6P5rFLmYH)
- [AI/ML Seminar Series: Ruiqi Gao (2/14/2022) - YouTube](https://www.youtube.com/watch?v=eAozs_JKp4o)
[Alex Graves, IJCNN 2017 Plenary Talk: Frontiers in Recurrent Neural Network Research Pt. 1 - YouTube](https://youtu.be/pwyBoPFsQ4M?si=H2t6w7vxy0cAqEjs)


##topics
diffusion model
[AI/ML Seminar Series: Ruiqi Gao (2/14/2022) - YouTube](https://www.youtube.com/watch?v=eAozs_JKp4o)

Design a study
- clearly write why you are doing the study
	-  and why - what do you want to learn not external factor like boss or need to do it cuz of formality etc
- get reading - literature review - others know more about your intended research - not just you - learn from them
	-  read the abstracts if access is an issue
- purpose - reflect on lit review - what questions and what answers you
- research questions - awesome program vs people's self confidence
- define your terms - definition of every big word in your research / lit review what you mean by confidence - as specific as possible
- make a list of data you need to answer the research questions - who collects it or someone else collect it
- look at each type of data - will i be using it for:
	- describe 
	- relationships
	- impact
- choose a research design
	- impact, cause or effect -experiment or quasi experiment
		- can you randomly assign people or things to different groups - experimental group and controll group - maybe multiple expermintal and controls
	- relationship - correlational study
	- describe, describe, describe - descriptive study
	- meta analysis - no need of this series
- population and sample
	- population people organisations entity things organizations you are interested about learning about
	- specific criterai youw ant people or thigns to have
	- age, experience, income, teacher (which country, what grades, which schools, what they teach matter? etc)
	- as specific as possible about the population
- sample
	- challenging to collect data on everyone in your population
	- in business or small org sample can be population
	- recruitment sample - you are all eligible please be in my study
	- study sample - the people who say yes
	- be able to defend - tight process 
	- have a strategy
	- build a good case, argument why you did it the way you did it defend choices - sharing links on tiktok or internet is less credibvle
	- like i partenered with this org cuz they work with the kind of people i want in my study
	- recruitment criteria
	- pathways to get access to the people i want  in my study - have a rec strat for each pathway
	- shjouldnt be ins tud if they dont share the characteristics
- sampling strategy: sample represnetative of popluation so what i studied about sample represents whole population
	- convenience
		- snowball
	- random sampling
		- stratified - tight and neat - if not equal representation
	- everyone - invite all
- sample size - trasnparent reporting
	- statistically significant results
	- sample size calculator online
- instruments 
	- survey design
	- tools
	- observation protocol
	- interview questions
	- see the places where you need to get the data - you need to collect 
	- maybe multiple surveys
	- what you need to collect data - maybe a alot of ways - be comprehensive
	- all the data you need to request or download
	- how to get contact and sign agreement
- feasibiility
	- get real with your self
	- set yourself up for success
	- you canc hange the plan
	- omb i cant be designing 5 surveys rn
	- will be you be able to design all the intruments and get all those data
	- dont burn out or be stressed
	- some surveys are not necessary
	- some poeple have alotof money 200k from the gov and team
	- triaging
	- so oyu have the support
- timeline 
	- end in mind
	- final report
	- then what do you need to do the final report - then when do you need to start colelcting the data
	- then what date do you need to deign the isnturments
	- when will you indeitfy the sample
	- start with a realistic timeline
	- start to finish 
- start executing - design data collecction tool
- design data collection tols
	- guide by research question
	- look at research qurestion to connect with tool
	- what ninfo do i need fromt his survey in order to answer this question
	- big broad strokes - make resarch quresiton a litlle more spefiic o measurbale 
- take a look at different examples of similar survey / studies - look for inspiration - dont copy - look at many - make a messy draft 
- review questions
	- ask family or friends to review it
	- draft a data collection tool
	- be critical - does it make sense is it confusing
- Pilot the data collection tool
	- trial run
	- follow up questions how can i improve the data tool during trial run so that i can get the data which i need
	- dont go to same place but similar place for data collection pilor
- are you doing research or evaluation?
	- little r or big R
	- institution review board - plan for review - informed consent preferable for all data collection
	- write your statement for your nformed consent 
- strategic plan to recruit people for study group
	- how do i get to people 
	- not tweeting or sharing link
	- limit control 
	- tigther more well defined groupo 
	- go to chamber of commerce to find 30 yo entrepreneurs instead of intenret
	- speicifc reddit r facebook gourp is okay too no right ans but need to be intenetion and able to defend
- follow the schedule to finish study
	- meaningful and intentional
	- be flexible if not working
	- give yourself grace
- data analysis
	- all roads lead back to your research questions
	- this data is going to be used for xyz - for relations, narrative story, regressional, enova t test manova mancova
	- document alot - take the best notes - keep the filings sharp - cpature anyhting made any change to data
- write out your findings 
	- semi confidential tip: best way to organisation - based on research question - simplify research questions and if they overlap or put them in one place and later say how they fit otgether
- sharing your data 
	- adequetely desribe who is inyour study
	- study protocol - survey questions what were you counting - desicisons and what defines a decision
	- make your study available
	- preprint or open access
	- be a cool researcher who can use your data and make sense f it and no wriiten so no one can understadn or it is hidden behind a paywall where no one read it


































**The "Hidden Gem":** Look into **MLSys (Conference on Machine Learning and Systems)**. It is the #1 venue specifically for "Efficient AI" and is highly respected by Google/Meta/Nvidia researchers.

### High-Prestige AI Conferences (The "Real" Deadlines)

| **Venue**        | **Focus Area**                                    | **2026 Submission Deadline** | **Status**                                           |
| ---------------- | ------------------------------------------------- | ---------------------------- | ---------------------------------------------------- |
| **ECCV 2026**    | **Computer Vision**, Super Resolution, Detection. | **March 5, 2026**            | **CRITICAL** for CV researchers.                     |
| **ICML 2026**    | **LLMs**, Agentic AI, Efficiency, Theory.         | **Late January 2026**        | (Check for "late-breaking" or workshop tracks).      |
| **KDD 2026**     | **Data Science** and AI for Science.              | **February 8, 2026**         | Good for large-scale efficient systems.              |
| **IJCAI 2026**   | General AI, **Agentic AI**, LLMs.                 | **January 19, 2026**         | (Passed, but check for workshop deadlines in March). |
| **NeurIPS 2026** | All AI (The "Superbowl" of AI).                   | **~May 2026**                | You have time to prep this one!                      |
### 2026 Target Journals (Nature Portfolio)

| **Journal**                     | **Best Fit For...**                                                   | **Deadlines**                           | **Expert Tip**                                                              |
| ------------------------------- | --------------------------------------------------------------------- | --------------------------------------- | --------------------------------------------------------------------------- |
| **Nature Machine Intelligence** | High-level breakthroughs in **Agentic AI** or LLM reasoning.          | Rolling (Year-round)                    | Very selective. Focus on the _impact_ of your agent.                        |
| **Nature Communications**       | **Object Detection** or **Super Resolution** with real-world impact.  | Rolling (Year-round)                    | Great for multidisciplinary AI (e.g., AI for medicine/climate).             |
| **Nature Sensors** (New 2026!)  | **Efficient CV** or **Super Res** running on actual hardware/sensors. | **Open Now**                            | Being a "New Launch," they are hungry for high-quality submissions.         |
| **Nature Electronics**          | **Efficient systems**, faster chips, or low-memory hardware AI.       | Rolling (Year-round)                    | Focus heavily on the "Systems" and "Memory" hardware specs.                 |
| **Scientific Reports**          | Solid, technically correct work in **Object Detection** or **LLMs**.  | **April 17, 2026** (Special Collection) | There is a current call for "Deep Learning for Real-Time Object Detection." |






Collection 
# Deep learning for real-time object detection
Submission status
Open
Submission deadline
17 April 2026
https://www.nature.com/collections/hahafahbdj

# swarm robotics
Emphasising decentralisation, it enables robots to accomplish complex tasks through local collaboration. Inspired by recent advances in AI and machine learning,

**Nature Sensors** (New 2026!)**Efficient CV** or **Super Res** running on actual hardware/sensors.

sensors - ai agentic vsr cot object detection prompt free training free

**"Pruning," "Quantization,"** or **"Knowledge Distillation."**



arxiv referral

## . The "Springer Nature" Ecosystem

Since you have a **Nature waiver**, you are dealing with **Springer Nature**, one of the "Big Three" publishers.

- **The Scope:** Every journal has a "Scope" document. If your AI paper is about _Efficiency_ but the journal's scope is _Social Impacts of AI_, you will get a **Desk Reject**.
    
- **The Transfer Desk:** If you submit to a top Nature journal and they reject you, they may offer to "transfer" your paper to a "sister" journal (like _Scientific Reports_) along with the reviews you've already received.



- **The PRISMA Protocol:** If you are writing a review, you _must_ follow the **PRISMA 2020** statement. It includes a 27-item checklist to ensure your review is transparent and reproducible.
    
- **Reproducibility Statement:** Modern AI venues (like ICLR 2026) now often **require** a dedicated paragraph explaining how someone can replicate your "Efficient System" experiments.
    
- **Anonymity (Double-Blind):** Most top AI conferences (CVPR, NeurIPS) will **desk-reject** you immediately if your name or your lab's name appears anywhere in the paper or the code links during the first round of review.



##learn

lstm
vector calculus + checl cm2 contents - do questions



## The Peer Review Lifecycle

1. **Submission:** You send the manuscript.
    
2. **Desk Review:** The Editor looks at it. If it’s bad, you get a **Desk Reject** (no feedback).
    
3. **Peer Review:** 2–4 experts (Reviewers) read it.
    
4. **Decisions:**
    
    - **Accept:** Extremely rare on the first try.
        
    - **Minor Revision:** You’re 90% there. Just fix small things.
        
    - **Major Revision:** They like the idea, but you need more experiments.
        
    - **Reject & Resubmit:** They hate the current paper but think the data is okay if you rewrite everything.
        
    - **Reject:** Game over. Go to a different journal.
## Summary of Citations & Standards

- **Standard for Systematic Reviews:** **PRISMA 2020 Statement** (Page et al., _The BMJ_). You must cite this if you write a review.
    
- **Standard for AI Reporting:** **CONSORT-AI** or **SPIRIT-AI** (Cross-disciplinary standards for reporting AI clinical trials/experiments).
    
- **Ethical Standard:** **COPE (Committee on Publication Ethics)**. Most journals follow their rules for what counts as "Plagiarism."
## 5. The "Nature" Ecosystem

"Nature" is not just one journal; it is a **Portfolio**.

- **Nature (The Flagship):** General science. Only for "World-Firsts."
    
- **Nature [Discipline]:** e.g., _Nature Machine Intelligence_. Q1, ultra-prestigious.
    
- **Nature Communications:** High-impact, open-access, very expensive.
    
- **Scientific Reports:** The "Filter" journal. It is Q1/Q2 but accepts anything that is scientifically correct.

https://orcid.org/0009-0001-1478-6206



**The Point:** Funding agencies and universities often _only_ count Q1 or Q2 publications for your PhD or promotions.


Rank	Percentile	Meaning
Q1	Top 25%	The "Elite." High prestige, very hard to get into.
Q2	25% to 50%	High-quality, reliable research.
Q3	50% to 75%	Average. Good for "filling out" a CV.
Q4	Bottom 25%	Often new journals or those with very low impact.


- **Web of Science (WoS):** The most prestigious, selective database. Run by _Clarivate_. Getting a paper "Indexed in WoS" is the gold standard for tenure.
    
- **Scopus:** Run by _Elsevier_. Larger than WoS, covers more journals, and is the source for the "CiteScore" metric.
    
- **Google Scholar:** The "wild west." It indexes everything (blogs, preprints, slide decks), so its citation counts are always higher but considered less "official" than WoS/Scopus.
    
- **Springer / Elsevier / Wiley:** These are **Publishers**, not databases. They are the "Big Three" corporations that own thousands of journals (like Sony or Universal in music).



##note
Your **H-index = h** if you have **h papers that each have at least h citations**.

Even though you have 60 papers, the H-index does not reward quantity alone. It rewards **consistent citation impact**.

Notice:

- Many low-cited papers → low H-index
    
- Fewer highly cited papers → higher H-index
    

The H-index balances **productivity (number of papers)** and **impact (citations)**.


1. The "Big Three" Review Types
![[Pasted image 20260221072437.png]]

- **Narrative Review:** A broad, non-technical "story" of a field. **Subjective.** High risk of bias because you only pick the papers you like.
    
- **Systematic Review (SR):** The gold standard. You follow a **strict protocol** (like **PRISMA**) to find _every_ relevant paper using specific keywords. It must be replicable.
    
- **Meta-Analysis:** The math version of an SR. You take the numerical data from 20 different papers and run a statistical test to find one "Grand Truth."
    
    - _Expert Tip:_ Use the **Forest Plot** to visualize this.


## 2. Bibliometric Terms (The Data of Science)

**Bibliometrics** is the statistical analysis of books, articles, or other publications.

- **Scientometrics:** Measuring the impact of science itself (growth of AI, etc.). #imp
    
- **Impact Factor (IF):** A journal's score. Higher = more "prestigious" (Nature is ~60+, PLOS ONE is ~3).
    
- **H-index:** A researcher's score. If your H-index is 10, you have 10 papers with at least 10 citations each.
    
- **Citation Mining:** * **Forward Snowballing:** Looking at who cited your paper _after_ it was published. #imp
    
    - **Backward Snowballing: Looking at the reference list of a paper to find older work.** #imp
        
- **Co-Citation Analysis: If Paper A and Paper B are both cited by Paper C, they are likely related.** #imp

**Grey Literature:** Research not published in commercial journals (e.g., government reports, white papers, theses, or **arXiv** preprints).

**Preprint:** A version of your paper uploaded to a server (arXiv, BioRxiv) _before_ peer review. **Crucial in AI.**

**Double-Blind Review:** The reviewers don't know who you are, and you don't know who they are.

**Desk Reject:** When an editor rejects your paper in 24 hours without even sending it to reviewers (the "Nature" special).

**Salami Slicing:** (Avoid this!) Breaking one big research project into many tiny, "thin" papers to boost your publication count.

If you want to be a "First Author" quickly, a **Bibliometric Analysis** of "Efficient AI Agents" is a great way to start. You don't need to run a single experiment; you just need to analyze the data of existing papers. #imp 

Would you like me to find a "PRISMA" template so you can see how a systematic review is actually structured?



## 1. The "Nature" Tier: Where to Use Your Waiver

Your friend's waiver likely applies to the **Nature Portfolio**. For AI research that focuses on efficiency and agents, these are your best bets:

- **Nature Machine Intelligence (The Dream):** This is the flagship AI journal. They love papers that solve fundamental problems, like how to make agents reason efficiently or how to run complex vision models on low-power hardware.
    
- **Nature Communications (The High-Impact Choice):** This is a multidisciplinary journal. It’s perfect if your "efficient system" has a big application—for example, an AI agent that runs on a drone for environmental monitoring or a tiny LLM for medical devices.
    
- **Scientific Reports (The "Safety" Nature Choice):** Also part of the Nature family, this is much **easier** to get into. It doesn't require "world-changing" results, just solid, technically correct science. If your goal is just to have the Nature logo on your first paper, this is the most accessible path.
    

### New for 2026: Nature Sensors

As you are working on efficient systems, keep an eye on the newly launched **Nature Sensors (2026)**. They are specifically looking for AI that integrates with hardware/sensors, which often requires exactly the "less memory/faster" optimizations you are building.

---

## 2. Best "Easiest" AI Journals (Fast & High Acceptance)

If Nature feels too high-pressure for your first attempt, these journals are known for being author-friendly and efficient:

|**Journal**|**Why it's "Easy"**|**Best For...**|
|---|---|---|
|**IEEE Access**|Binary "Yes/No" peer review; very fast.|Efficient CV or hardware-related AI.|
|**Pattern Recognition Letters**|Faster than the main journal; good for CV.|Specific Computer Vision "tricks" or optimizations.|
|**Applied Intelligence**|Broad scope; high volume of papers.|Practical applications of Agentic AI.|
|**Sensors (MDPI)**|Extremely fast (3-6 weeks).|AI for edge devices and vision.|

---

## 3. The "Conference" Reality in AI

In AI, **Conferences are often more prestigious than Journals.** If your work is truly "Faster and Less Memory," the AI community might actually respect a paper in these venues _more_ than a journal:

- **CVPR / ICCV:** The "Gold Standard" for Computer Vision.
    
- **NeurIPS / ICML:** The top tier for LLM and efficient architecture research.
    
- **SysML (MLSys):** This is **your perfect niche**. It specifically focuses on the intersection of Systems and Machine Learning (efficiency, memory, speed).
    

---

## 4. How to Frame Your "Efficiency" Paper

To get into a high-tier journal like _Nature Machine Intelligence_ with an efficiency paper, you need to show more than just "it's 10% faster." You need to show **why** that matters.

> **Example Narrative:** "By reducing memory by 80%, we enable autonomous agents to perform real-time reasoning on $5 devices, which was previously impossible."
## 1. Types of Research Writing

- **Journal Paper:** The "Final Boss" of research. It’s a long, detailed document (often 10–20+ pages) published in a periodic magazine (like _Nature_). It’s considered the most permanent and rigorous record.
    
- **Review Paper:** You don't present _new_ data. Instead, you summarize and critique all the existing papers on a topic (e.g., "A Review of Efficient LLM Architectures 2023-2026"). These get cited a lot!
    
- **Letter (or Communication):** A very short journal paper (2–4 pages) meant for **urgent**, groundbreaking results. _Nature_ actually calls many of its primary research papers "Letters."
    
- **Short vs. Long Paper:** Common in conferences.
    
    - **Long:** Full work with complete experiments (usually 8–10 pages).
        
    - **Short:** Work-in-progress, a small "trick," or a specific narrow finding (usually 3–5 pages).
        
- **Book Chapter:** An invited piece for a specific book. In AI, these are less "prestigious" than papers because they aren't always peer-reviewed as strictly, but they are great for deep-dives into a niche.
    

---

## 2. Conferences vs. Journals (The AI Split)

This is where AI is unique. In most fields, conferences are just for "talking." In AI, **conferences are where the real publishing happens.**

- **Proceeding:** This is the "book" or digital collection of all the papers accepted at a conference. If you get into CVPR, your paper is published in the "CVPR 2026 Proceedings."
    
- **Can a conference paper be a journal paper?** Yes, but with a catch. You usually cannot publish the **exact same** paper in both.
    
    - **The "Extension" Rule:** Typically, you publish a "Conference Paper" first to get the word out quickly. Later, you add 30–40% _new_ content/data and submit the "Extended Version" to a **Journal**.
        
- **Timeline:** * **Conference:** Fast. Submit $\rightarrow$ 3 months to review $\rightarrow$ 1 month to fix $\rightarrow$ Published.
    
    - **Journal:** Slow. Submit $\rightarrow$ 6–12 months of back-and-forth reviews $\rightarrow$ Published.
        

---

## 3. Money: Open vs. Closed Access

- **Closed Access (Traditional):** **Free for you to publish**, but readers (or their libraries) have to pay to read it.
    
- **Open Access (OA):** **You pay** a fee (APC - Article Processing Charge), and the paper is free for the whole world to read.
    
- **Typical Fees (2026):**
    
    - **Low-tier:** $500 – $1,500.
        
    - **Mid-tier (IEEE, MDPI):** $2,000 – $3,500.
        
    - **Elite (Nature/Science):** $6,000 – $11,000. _(This is where your friend's waiver is huge!)_
        

---

## 4. Scoping

"Scoping" has two meanings depending on context:

1. **Project Scoping:** Deciding the boundaries of your paper. (e.g., "I am only looking at vision-based agents, not voice-based ones.")
    
2. **Scoping Review:** A specific type of paper that "maps" a new field to see what research even exists. It’s broader and less "picky" than a Systematic Review.

doubts:
Systematic Review
review paper short long book chapter letter proceeding journal conference - conference stuff published in journal or proceedings - open access - typical fees - closed access - timelines - scoping - explain each word i have doubts in these


##topics
superresolution
vision agents
other than coding agents
COT
agentic pipelines




##conferences
CVPR / ICCV
NeurIPS / ICML
SysML (MLSys)

##new
Nature Health
Nature Sensors

##journal
**Nature Communications
Nature Machine Intelligence
Sensors (MDPI)
Applied Intelligence
Pattern Recognition Letters
IEEE Access

##blog
medium
personal site
linkedin


**"Baseline" you are comparing your work against?** (e.g., "My model is faster than MobileNetV4"

**Example Narrative:** "By reducing memory by 80%, we enable autonomous agents to perform real-time reasoning on $5 devices, which was previously impossible."

##where to publish


1. The "Nature" Tier: Where to Use Your Waiver

Your friend's waiver likely applies to the **Nature Portfolio**. For AI research that focuses on efficiency and agents, these are your best bets:

- **Nature Machine Intelligence (The Dream):** This is the flagship AI journal. They love papers that solve fundamental problems, like how to make agents reason efficiently or how to run complex vision models on low-power hardware.
    
- **Nature Communications (The High-Impact Choice):** This is a multidisciplinary journal. It’s perfect if your "efficient system" has a big application—for example, an AI agent that runs on a drone for environmental monitoring or a tiny LLM for medical devices.
    
- **Scientific Reports (The "Safety" Nature Choice):** Also part of the Nature family, this is much **easier** to get into. It doesn't require "world-changing" results, just solid, technically correct science. If your goal is just to have the Nature logo on your first paper, this is the most accessible path.
    

### New for 2026: Nature Sensors

As you are working on efficient systems, keep an eye on the newly launched **Nature Sensors (2026)**. They are specifically looking for AI that integrates with hardware/sensors, which often requires exactly the "less memory/faster" optimizations you are building.