### **1. Experimental Variables (The "What we are testing")**

In a scientific experiment, you must clearly define what you change, what you measure, and what you keep constant.

| **Variable Type**             | **Definition in Your Context**                         | **Examples (Your Project)**                                                                                                                                                                                                                                                                    |
| ----------------------------- | ------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Independent Variable (IV)** | _The factor you manipulate or change._                 | 1. **System Architecture:** (Synchronous vs. Asynchronous Producer-Consumer)<br><br>  <br><br>2. **Sampling Method:** (Fixed Stride vs. Adaptive Keyframe Sampling)<br><br>  <br><br>3. **Model Variant:** (Standard YOLOv26 vs. RexOmni)                                                      |
| **Dependent Variable (DV)**   | _The outcome you measure to see the effect of the IV._ | 1. **Throughput:** FPS (Frames Per Second)<br><br>  <br><br>2. **Latency:** Milliseconds (ms) per frame<br><br>  <br><br>3. **Accuracy:** mAP@50 (Mean Average Precision) and Recall@1 (Person Search)<br><br>  <br><br>4. **Resource Usage:** VRAM (MB) and Est. Power (Watts)                |
| **Control Variable (CV)**     | _Factors kept constant to ensure a fair test._         | 1. **Hardware:** Google Colab T4 GPU (Must verify via `!nvidia-smi`)<br><br>  <br><br>2. **Dataset:** LongVideoBench (Same video files for all tests)<br><br>  <br><br>3. **Input Resolution:** Fixed at 640x640 (or 1280x720)<br><br>  <br><br>4. **Framework:** PyTorch version & Batch size |

---

### **2. Research Hypotheses (The "Predictions")**

You need formal predictions to prove you aren't just "hacking" but doing science.

- **Null Hypothesis ($H_0$):** There is **no significant difference** in throughput (FPS) or accuracy (mAP) between the standard synchronous YOLOv26 pipeline and the proposed asynchronous RexOmni pipeline.
    
- **Alternative Hypothesis ($H_1$):** The proposed asynchronous pipeline with Adaptive Keyframe Sampling will increase throughput by **>200%** compared to the baseline while maintaining a degradation of **<1% mAP** on the VisDrone dataset.
    

---

### **3. Ablation Study Plan (The "Proof of Contribution")**

An ablation study removes parts of your system to prove _which specific part_ caused the improvement. You must plan this _now_ so you code the ability to turn features on/off.

- **Ablation 1 (No Async):** Run RexOmni _without_ the Producer-Consumer buffer (Synchronous). _Goal: Prove the speedup comes from architecture, not just the model._
    
- **Ablation 2 (No AKS):** Run the system with _Fixed Stride_ (skip every 5 frames) instead of Adaptive Sampling. _Goal: Prove the accuracy gain comes from your smart sampling._
    
- **Ablation 3 (No Zero-Shot):** Run the search using only Face ID (traditional) vs. your Text Prompt. _Goal: Prove the flexibility of your method._
    

---

### **4. Inclusion & Exclusion Criteria (The "Boundaries")**

Define exactly what data enters your study to avoid "cherry-picking" bias.

- **Inclusion Criteria:**
    
    - Videos must be **>15 minutes** in duration (to test long-term memory/drift).
        
    - Videos must contain **at least 3** distinct "target persons" entering/exiting the frame.
        
    - Footage must include **dynamic backgrounds** (e.g., swaying trees, moving cars) to test false positives.
        
- **Exclusion Criteria:**
    
    - Night-vision or IR footage (unless you specifically train for it).
        
    - Videos with **<720p resolution** (too low for attribute recognition).
        
    - Corrupted video files or variable framerate (VFR) clips that mess up timing benchmarks.
        

---

### **5. Ethical & Reproducibility Statement**

Since you are dealing with surveillance, you **must** include this in your "Methodology" section to get published.

- **Ethics:** "All benchmarks are conducted on public datasets (VisDrone/LongVideoBench). No private surveillance data or personally identifiable information (PII) of non-consenting individuals is used."
    
- **Reproducibility:** "We fix the random seed (`torch.manual_seed(42)`) and lock the Colab GPU type to NVIDIA T4 to ensure all results can be replicated by other researchers."
    

### **Next Step: The Code**

Now that the variables are defined, the next logical step is to write the **Baseline Benchmarking Script** for Google Colab.

### **1. Experimental Variables (The "What we are testing")**

In a scientific experiment, you must clearly define what you change, what you measure, and what you keep constant.

|**Variable Type**|**Definition in Your Context**|**Examples (Your Project)**|
|---|---|---|
|**Independent Variable (IV)**|_The factor you manipulate or change._|1. **System Architecture:** (Synchronous vs. Asynchronous Producer-Consumer)<br><br>  <br><br>2. **Sampling Method:** (Fixed Stride vs. Adaptive Keyframe Sampling)<br><br>  <br><br>3. **Model Variant:** (Standard YOLOv26 vs. RexOmni)|
|**Dependent Variable (DV)**|_The outcome you measure to see the effect of the IV._|1. **Throughput:** FPS (Frames Per Second)<br><br>  <br><br>2. **Latency:** Milliseconds (ms) per frame<br><br>  <br><br>3. **Accuracy:** mAP@50 (Mean Average Precision) and Recall@1 (Person Search)<br><br>  <br><br>4. **Resource Usage:** VRAM (MB) and Est. Power (Watts)|
|**Control Variable (CV)**|_Factors kept constant to ensure a fair test._|1. **Hardware:** Google Colab T4 GPU (Must verify via `!nvidia-smi`)<br><br>  <br><br>2. **Dataset:** LongVideoBench (Same video files for all tests)<br><br>  <br><br>3. **Input Resolution:** Fixed at 640x640 (or 1280x720)<br><br>  <br><br>4. **Framework:** PyTorch version & Batch size|

---

### **2. Research Hypotheses (The "Predictions")**

You need formal predictions to prove you aren't just "hacking" but doing science.

- **Null Hypothesis ($H_0$):** There is **no significant difference** in throughput (FPS) or accuracy (mAP) between the standard synchronous YOLOv26 pipeline and the proposed asynchronous RexOmni pipeline.
    
- **Alternative Hypothesis ($H_1$):** The proposed asynchronous pipeline with Adaptive Keyframe Sampling will increase throughput by **>200%** compared to the baseline while maintaining a degradation of **<1% mAP** on the VisDrone dataset.
    

---

### **3. Ablation Study Plan (The "Proof of Contribution")**

An ablation study removes parts of your system to prove _which specific part_ caused the improvement. You must plan this _now_ so you code the ability to turn features on/off.

- **Ablation 1 (No Async):** Run RexOmni _without_ the Producer-Consumer buffer (Synchronous). _Goal: Prove the speedup comes from architecture, not just the model._
    
- **Ablation 2 (No AKS):** Run the system with _Fixed Stride_ (skip every 5 frames) instead of Adaptive Sampling. _Goal: Prove the accuracy gain comes from your smart sampling._
    
- **Ablation 3 (No Zero-Shot):** Run the search using only Face ID (traditional) vs. your Text Prompt. _Goal: Prove the flexibility of your method._
    

---

### **4. Inclusion & Exclusion Criteria (The "Boundaries")**

Define exactly what data enters your study to avoid "cherry-picking" bias.

- **Inclusion Criteria:**
    
    - Videos must be **>15 minutes** in duration (to test long-term memory/drift).
        
    - Videos must contain **at least 3** distinct "target persons" entering/exiting the frame.
        
    - Footage must include **dynamic backgrounds** (e.g., swaying trees, moving cars) to test false positives.
        
- **Exclusion Criteria:**
    
    - Night-vision or IR footage (unless you specifically train for it).
        
    - Videos with **<720p resolution** (too low for attribute recognition).
        
    - Corrupted video files or variable framerate (VFR) clips that mess up timing benchmarks.
        

---

### **5. Ethical & Reproducibility Statement**

Since you are dealing with surveillance, you **must** include this in your "Methodology" section to get published.

- **Ethics:** "All benchmarks are conducted on public datasets (VisDrone/LongVideoBench). No private surveillance data or personally identifiable information (PII) of non-consenting individuals is used."
    
- **Reproducibility:** "We fix the random seed (`torch.manual_seed(42)`) and lock the Colab GPU type to NVIDIA T4 to ensure all results can be replicated by other researchers."
    

### **Next Step: The Code**

Now that the variables are defined, the next logical step is to write the **Baseline Benchmarking Script** for Google Colab.

[Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability](https://arxiv.org/html/2411.04008)


#todo

Benchmark of baseline
Literature review + systematic review
Meta analysis
Proposed method and implementation
Benchmark of method - experiment
Ablation studies
Conclusion



### standard person detection (bounding box only)

- **# models:** YOLOv8, YOLOv11, YOLOv26, RF-DETR
    
- **# datasets:** COCO (Person class), CrowdHuman, VisDrone
    
- **# metrics to measure:** mAP@50, mAP@50-95, Latency (ms)
    
- **# control - for now only standard control images dataset to get frames per second:** Fixed resolution images (e.g., 640x640) from a standard dataset on a controlled GPU (e.g., Colab T4) to isolate and measure pure inference FPS throughput.
    

### standard person tracking (temporal ID assignment / MOT)

- **# models:** YOLOv26 + ByteTrack, RF-DETR + DeepSORT (or embedded tracking heads)
    
- **# datasets:** MOT17, MOT20
    
- **# metrics to measure:** MOTA (Multi-Object Tracking Accuracy), IDF1, HOTA
    
- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 resolution image sequences on a controlled GPU to measure pure inference + tracker update FPS.
    

### standard person recognition (embedding match - direct face/body feature vectors)

- **# models:** InsightFace (Buffalo_l), DeepFace, OSNet, ResNet50-ReID
    
- **# datasets:** Market-1501, LFW (Labeled Faces in the Wild)
    
- **# metrics to measure:** Rank-1 Accuracy, Rank-5 Accuracy, mAP (Retrieval)
    
- **# control - for now only standard control images dataset to get frames per second:** Standard cropped bounding box images (e.g., 128x256 for bodies, 112x112 for faces) on a controlled GPU to measure pure embedding extraction FPS.
    

### standard person recognition (multi-attribute match classification - gender, clothing, age)

- **# models:** PAR (Pedestrian Attribute Recognition) networks like StrongBaseline, modified YOLOv26 with a Multi-Task Head
    
- **# datasets:** PETA, PA-100K
    
- **# metrics to measure:** mA (mean Accuracy), F1-score (Instance-level)
    
- **# control - for now only standard control images dataset to get frames per second:** Standard cropped bounding box images (e.g., 128x256) on a controlled GPU to measure multi-head classification FPS.
    

### zero-shot prompt person recognition (open vocabulary text attribute only)

- **# models:** YOLO-World, Grounding DINO, RexOmni (proposed text-only branch)
    
- **# datasets:** CUHK-PEDES, UFine6926 (new ultra-fine benchmark), RSTPReid
    
- **# metrics to measure:** Recall@1, Recall@5, mAP, mSD (mean Similarity Distribution)
    
- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 images + fixed-length text prompts on a controlled GPU to measure combined vision-language inference FPS.
    

### one-shot prompt person recognition (single reference image)

- **# models:** RexOmni (image branch), ZERO (Visual Exemplars), modified YOLOE
    
- **# datasets:** PRW (Person Search in the Wild), CUHK-SYSU
    
- **# metrics to measure:** mAP (Search), Top-1 Accuracy
    
- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 gallery images + one 128x256 query image on a controlled GPU to measure prompt-matching FPS.
    

### few-shot prompt person recognition (multiple reference images)

- **# models:** RexOmni, Few-Shot Object Detectors (FSOD architectures)
    
- **# datasets:** PRW (custom few-shot splits)
    
- **# metrics to measure:** mAP, Recall@K
    
- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 gallery images + fixed $K$ query images (e.g., $K=3$) on a controlled GPU to measure multi-reference matching FPS.
    

### few-shot multi-modal person recognition (images + text attributes)

- **# models:** RexOmni (Full hybrid), BLIP-2 / PaliGemma (adapted for search)
    
- **# datasets:** UFine3C (cross-domain/granularity benchmark), custom SurveillanceVQA splits
    
- **# metrics to measure:** Multi-modal Recall@1, mAP
    
- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 gallery images + text prompt + $K$ query images on a controlled GPU to measure multi-modal fusion FPS.
    

### action-conditioned person detection (behavior/pose prompt matching)

- **# models:** YOLOv26-Pose + VLM, VideoScan, SlowFast
    
- **# datasets:** AVA (Atomic Visual Actions), UCF101
    
- **# metrics to measure:** Spatio-temporal mAP, Frame-level mAP
    
- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 image sequences (or batched frames) on a controlled GPU to measure spatio-temporal inference FPS.
    

### cross-camera person re-identification (ReID across disjoint spatial views)

- **# models:** OSNet, TransReID, RexOmni (multi-view embedding)
    
- **# datasets:** MSMT17, Market-1501
    
- **# metrics to measure:** mAP, Rank-1, Rank-5
    
- **# control - for now only standard control images dataset to get frames per second:** Standard cropped bounding box images (e.g., 128x256) on a controlled GPU to measure cross-camera embedding comparison FPS.










#task1
### standard person detection (bounding box only)

**# models (Exhaustive List by Family):**

- **The YOLO Family (One-Stage CNNs):** YOLOv8, YOLOv9, YOLOv10, YOLOv11, YOLOv12, YOLOv26. _(Best for: Absolute real-time speed, low latency, and NMS-free edge deployment)._
    
- **The DETR Family (Vision Transformers):** DETR, Deformable DETR, RT-DETR, RT-DETRv2, RF-DETR (Roboflow), D-FINE. _(Best for: End-to-end detection without anchors, high accuracy on complex/crowded scenes)._
    
- **Vision-Language / Next-Point Prediction (MLLMs):** Rex-Omni (IDEA Research), YOLO-World, Grounding DINO, GLIP. _(Best for: Zero-shot flexibility. For this specific task, they act as standard detectors by simply hardcoding the text prompt to "person")._
    
- **The R-CNN Family (Two-Stage Detectors):** Faster R-CNN, Mask R-CNN (bounding box branch), Cascade R-CNN. _(Best for: Absolute precision baselines, though typically too slow for 500+ FPS pipelines)._
    
- **Lightweight/Mobile CNNs:** SSD (MobileNet-SSD), RetinaNet, EfficientDet (D0-D7), CenterNet. _(Best for: Highly constrained edge hardware, serving as parameter-count baselines)._
    

**# datasets:**

- **General Purpose:** COCO (filtered for the 'person' category - 80 classes reduced to 1), PASCAL VOC.
    
- **Dense & Crowded Scenes:** CrowdHuman, WiderPerson. _(Crucial to test if the model drops bounding boxes when people overlap)._
    
- **Surveillance & Aerial:** VisDrone (high-angle, tiny objects), Caltech Pedestrian, CityPersons.
    

**# metrics to measure:**

- **Accuracy:** mAP@50, mAP@50-95 (Mean Average Precision across standard IoU thresholds), F1-Score (balances false positives and false negatives).
    
- **Speed:** End-to-end Latency (ms) per frame, FPS (Frames Per Second).
    
- **Efficiency (The MLSys/Nature standard):** Parameter count (Millions), FLOPs/TFLOPs (computational cost), Peak VRAM Usage (MB), Power consumption (Watts/Joules).
    

**# control (for measuring pure inference FPS throughput):**

- **Hardware:** Fixed to a specific, identical GPU environment across all tests (e.g., Google Colab NVIDIA T4 or L4).
    
- **Input Data:** Standard, identical 640x640 resolution images drawn from a predefined validation split (e.g., COCO val2017).
    
- **Environment Constraints:** Batch size locked to 1 (to simulate real-time video streaming latency), FP16 precision enabled consistently (via PyTorch AMP or TensorRT), and identical Confidence/IoU thresholds programmed across all model evaluation scripts.



##tasks

  
  

### standard person detection (bounding box only)

  

- **# models:** YOLOv8, YOLOv11, YOLOv26, RF-DETR

- **# datasets:** COCO (Person class), CrowdHuman, VisDrone

- **# metrics to measure:** mAP@50, mAP@50-95, Latency (ms)

- **# control - for now only standard control images dataset to get frames per second:** Fixed resolution images (e.g., 640x640) from a standard dataset on a controlled GPU (e.g., Colab T4) to isolate and measure pure inference FPS throughput.

  

### standard person tracking (temporal ID assignment / MOT)

  

- **# models:** YOLOv26 + ByteTrack, RF-DETR + DeepSORT (or embedded tracking heads)

- **# datasets:** MOT17, MOT20

- **# metrics to measure:** MOTA (Multi-Object Tracking Accuracy), IDF1, HOTA

- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 resolution image sequences on a controlled GPU to measure pure inference + tracker update FPS.

  

### standard person recognition (embedding match - direct face/body feature vectors)

  

- **# models:** InsightFace (Buffalo_l), DeepFace, OSNet, ResNet50-ReID

- **# datasets:** Market-1501, LFW (Labeled Faces in the Wild)

- **# metrics to measure:** Rank-1 Accuracy, Rank-5 Accuracy, mAP (Retrieval)

- **# control - for now only standard control images dataset to get frames per second:** Standard cropped bounding box images (e.g., 128x256 for bodies, 112x112 for faces) on a controlled GPU to measure pure embedding extraction FPS.

  

### standard person recognition (multi-attribute match classification - gender, clothing, age)

  

- **# models:** PAR (Pedestrian Attribute Recognition) networks like StrongBaseline, modified YOLOv26 with a Multi-Task Head

- **# datasets:** PETA, PA-100K

- **# metrics to measure:** mA (mean Accuracy), F1-score (Instance-level)

- **# control - for now only standard control images dataset to get frames per second:** Standard cropped bounding box images (e.g., 128x256) on a controlled GPU to measure multi-head classification FPS.

  

### zero-shot prompt person recognition (open vocabulary text attribute only)

  

- **# models:** YOLO-World, Grounding DINO, RexOmni (proposed text-only branch)

- **# datasets:** CUHK-PEDES, UFine6926 (new ultra-fine benchmark), RSTPReid

- **# metrics to measure:** Recall@1, Recall@5, mAP, mSD (mean Similarity Distribution)

- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 images + fixed-length text prompts on a controlled GPU to measure combined vision-language inference FPS.

  

### one-shot prompt person recognition (single reference image)

  

- **# models:** RexOmni (image branch), ZERO (Visual Exemplars), modified YOLOE

- **# datasets:** PRW (Person Search in the Wild), CUHK-SYSU

- **# metrics to measure:** mAP (Search), Top-1 Accuracy

- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 gallery images + one 128x256 query image on a controlled GPU to measure prompt-matching FPS.

  

### few-shot prompt person recognition (multiple reference images)

  

- **# models:** RexOmni, Few-Shot Object Detectors (FSOD architectures)

- **# datasets:** PRW (custom few-shot splits)

- **# metrics to measure:** mAP, Recall@K

- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 gallery images + fixed $K$ query images (e.g., $K=3$) on a controlled GPU to measure multi-reference matching FPS.

  

### few-shot multi-modal person recognition (images + text attributes)

  

- **# models:** RexOmni (Full hybrid), BLIP-2 / PaliGemma (adapted for search)

- **# datasets:** UFine3C (cross-domain/granularity benchmark), custom SurveillanceVQA splits

- **# metrics to measure:** Multi-modal Recall@1, mAP

- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 gallery images + text prompt + $K$ query images on a controlled GPU to measure multi-modal fusion FPS.

  

### action-conditioned person detection (behavior/pose prompt matching)

  

- **# models:** YOLOv26-Pose + VLM, VideoScan, SlowFast

- **# datasets:** AVA (Atomic Visual Actions), UCF101

- **# metrics to measure:** Spatio-temporal mAP, Frame-level mAP

- **# control - for now only standard control images dataset to get frames per second:** Standard 640x640 image sequences (or batched frames) on a controlled GPU to measure spatio-temporal inference FPS.

  

### cross-camera person re-identification (ReID across disjoint spatial views)

  

- **# models:** OSNet, TransReID, RexOmni (multi-view embedding)

- **# datasets:** MSMT17, Market-1501

- **# metrics to measure:** mAP, Rank-1, Rank-5

- **# control - for now only standard control images dataset to get frames per second:** Standard cropped bounding box images (e.g., 128x256) on a controlled GPU to measure cross-camera embedding comparison FPS.












